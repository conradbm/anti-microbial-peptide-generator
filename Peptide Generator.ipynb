{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e8f41b-1cf4-49dd-a3f9-e244714e1df2",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5251e6ec-db77-4fe9-88d6-d624f75348bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 20\n",
      "Random Peptides: 59999\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "def generate_custom_sequence(sequence_length = 10):\n",
    "    # Define rules for your desired properties (e.g., hydrophobicity, charge)\n",
    "    # Implement logic to generate sequences accordingly\n",
    "    # ...\n",
    "# Define the amino acids (you can customize this list)\n",
    "\n",
    "    # For demonstration purposes, let's create a simple custom sequence\n",
    "    # Generate a random peptide sequence of length 10\n",
    "\n",
    "    random_sequence = \"\".join(random.choice(amino_acids) for _ in range(sequence_length))\n",
    "\n",
    "    return random_sequence\n",
    "\n",
    "def generate_peptides(n=100, sequence_length=10):\n",
    "    data=[]\n",
    "    for _ in range(n):\n",
    "        data.append(generate_custom_sequence(sequence_length = sequence_length))\n",
    "    return data\n",
    "\n",
    "custom_peptides = []\n",
    "vocab_size = len(set(amino_acids))\n",
    "print(\"Vocab Size:\", vocab_size)\n",
    "for seq_len in [6,8,10,12,15,20]:\n",
    "    [custom_peptides.append(i) for i in generate_peptides(n=10000, sequence_length=seq_len)]\n",
    "custom_peptides = list(set(custom_peptides))\n",
    "print(\"Random Peptides:\", len(custom_peptides))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f773eacf-e05d-4707-95eb-bd6936c6222d",
   "metadata": {},
   "source": [
    "### Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5b49cc4e-7206-472f-8a19-817c3cdebd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([59999, 20])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "class CharTokenizer:\n",
    "    def __init__(self, amino_acids):\n",
    "        self.char_to_index = {char: idx for idx, char in enumerate(amino_acids, start=1)}\n",
    "        self.index_to_char = {idx: char for char, idx in self.char_to_index.items()}\n",
    "        self.index_to_char[0] = \"\"\n",
    "        \n",
    "    def encode_many(self, sequences):\n",
    "        D = []\n",
    "        for seq in sequences:\n",
    "            D.append(torch.tensor([self.char_to_index[char] for char in seq]))\n",
    "\n",
    "        D_pad = pad_sequence(D, batch_first=True, padding_value=0)\n",
    "        return D_pad\n",
    "    def decode_many(self, indices):\n",
    "        D = []\n",
    "        for enc in indices:\n",
    "            D.append(\"\".join(self.index_to_char[idx.item()] for idx in enc))\n",
    "        return D\n",
    "        \n",
    "    def encode(self, sequence):\n",
    "        return [self.char_to_index[char] for char in sequence]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return \"\".join(self.index_to_char[idx] for idx in indices)\n",
    "\n",
    "# Example usage\n",
    "tokenizer = CharTokenizer(amino_acids)\n",
    "\n",
    "custom_peptides_encoded = tokenizer.encode_many(custom_peptides)\n",
    "# custom_peptides_decoded = tokenizer.decode_many(custom_peptides_encoded)\n",
    "print(custom_peptides_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bba1cfa-51c0-469e-8536-355a5d340499",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b415f0c7-ba76-4fe0-8c56-d33106b5c00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Loss: 2.6876\n",
      "Epoch 2/100 - Loss: 2.6795\n",
      "Epoch 3/100 - Loss: 2.6794\n",
      "Epoch 4/100 - Loss: 2.6794\n",
      "Epoch 5/100 - Loss: 2.6794\n",
      "Epoch 6/100 - Loss: 2.6793\n",
      "Epoch 7/100 - Loss: 2.6794\n",
      "Epoch 8/100 - Loss: 2.6793\n",
      "Epoch 9/100 - Loss: 2.6794\n",
      "Epoch 10/100 - Loss: 2.6794\n",
      "Epoch 11/100 - Loss: 2.6794\n",
      "Epoch 12/100 - Loss: 2.6793\n",
      "Epoch 13/100 - Loss: 2.6793\n",
      "Epoch 14/100 - Loss: 2.6792\n",
      "Epoch 15/100 - Loss: 2.6793\n",
      "Epoch 16/100 - Loss: 2.6793\n",
      "Epoch 17/100 - Loss: 2.6792\n",
      "Epoch 18/100 - Loss: 2.6793\n",
      "Epoch 19/100 - Loss: 2.6793\n",
      "Epoch 20/100 - Loss: 2.6789\n",
      "Epoch 21/100 - Loss: 2.6784\n",
      "Epoch 22/100 - Loss: 2.6782\n",
      "Epoch 23/100 - Loss: 2.6782\n",
      "Epoch 24/100 - Loss: 2.6782\n",
      "Epoch 25/100 - Loss: 2.6782\n",
      "Epoch 26/100 - Loss: 2.6782\n",
      "Epoch 27/100 - Loss: 2.6783\n",
      "Epoch 28/100 - Loss: 2.6782\n",
      "Epoch 29/100 - Loss: 2.6781\n",
      "Epoch 30/100 - Loss: 2.6782\n",
      "Epoch 31/100 - Loss: 2.6782\n",
      "Epoch 32/100 - Loss: 2.6782\n",
      "Epoch 33/100 - Loss: 2.6782\n",
      "Epoch 34/100 - Loss: 2.6782\n",
      "Epoch 35/100 - Loss: 2.6782\n",
      "Epoch 36/100 - Loss: 2.6782\n",
      "Epoch 37/100 - Loss: 2.6782\n",
      "Epoch 38/100 - Loss: 2.6782\n",
      "Epoch 39/100 - Loss: 2.6782\n",
      "Epoch 40/100 - Loss: 2.6782\n",
      "Epoch 41/100 - Loss: 2.6782\n",
      "Epoch 42/100 - Loss: 2.6782\n",
      "Epoch 43/100 - Loss: 2.6782\n",
      "Epoch 44/100 - Loss: 2.6781\n",
      "Epoch 45/100 - Loss: 2.6781\n",
      "Epoch 46/100 - Loss: 2.6781\n",
      "Epoch 47/100 - Loss: 2.6782\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[144], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output_size), targets\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     69\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlu\\lib\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlu\\lib\\site-packages\\torch\\autograd\\__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a simple LSTM model\n",
    "class PeptideLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, embedding_dim):\n",
    "        super(PeptideLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, hidden_state):\n",
    "        embedding_seq = self.embedding(input_seq)\n",
    "        lstm_out, hidden_state = self.lstm(embedding_seq, hidden_state)\n",
    "        output = self.fc(lstm_out)\n",
    "        output_probs = F.softmax(output, dim=-1)\n",
    "        return output_probs, hidden_state\n",
    "\n",
    "# Example usage\n",
    "input_size = vocab_size+1  # Number of unique amino acids\n",
    "hidden_size = 64\n",
    "output_size = vocab_size+1  # Same as input size for character-level prediction\n",
    "embedding_dim = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PeptideLSTM(input_size, hidden_size, output_size, embedding_dim)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (you'll need to adapt this to your dataset)\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "N = len(custom_peptides_encoded)\n",
    "num_batches = N // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    random_indices = torch.rand\n",
    "    \n",
    "    # Shuffle the dataset (optional but recommended)\n",
    "    random_indices = torch.randperm(len(custom_peptides_encoded))\n",
    "    shuffled_sequences = [custom_peptides_encoded[i] for i in random_indices]\n",
    "\n",
    "    for batch_start in range(0, len(shuffled_sequences), batch_size):\n",
    "        batch = shuffled_sequences[batch_start : batch_start + batch_size]\n",
    "        # Convert batch to PyTorch tensors\n",
    "        batch_tensors = torch.stack(batch, dim=0).long().to(device) # Assuming your sequences are integer indices\n",
    "\n",
    "        # Initialize hidden state (if using stateful LSTM)\n",
    "        hidden_state = None\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, hidden_state = model(batch_tensors, hidden_state)\n",
    "\n",
    "        # Compute loss (assuming your targets are the next characters in the sequences)\n",
    "        targets = batch_tensors[:, 1:]  # Shift targets by one position\n",
    "        loss = criterion(outputs[:, :-1].reshape(-1, output_size), targets.reshape(-1))\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / num_batches\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {average_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4286f888-5b7a-43ee-b91d-79dc5301bb5e",
   "metadata": {},
   "source": [
    "### Generate new sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2c54f101-c47e-44b8-a310-85d87d2273f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence: KKMVCRLQMVCWLQMVCWLQMVCWLQ\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "\n",
    "# Generating new sequences (after training)\n",
    "def generate_sequence(model, seed_sequence, length=20):\n",
    "    hidden_state = None\n",
    "    output_sequence = seed_sequence\n",
    "\n",
    "    for _ in range(length):\n",
    "        input_seq = tokenizer.encode_many([output_sequence[-1]])  # Convert last character to input\n",
    "        output, hidden_state = model(input_seq, hidden_state)\n",
    "        predicted_char = tokenizer.decode_many([output[-1].argmax(dim=1)])  # Sample from output distribution\n",
    "        output_sequence += predicted_char[0]\n",
    "\n",
    "    return output_sequence\n",
    "\n",
    "# Example usage\n",
    "seed_sequence = \"KKM\"  # Provide a starting sequence\n",
    "generated_sequence = generate_sequence(model, seed_sequence, length=25)\n",
    "print(f\"Generated sequence: {generated_sequence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821184fd-298f-4110-a128-72d369dd66be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
